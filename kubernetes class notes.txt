3/9/21	

	KUBERNETES

Kubernetes, also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications.

K8s:
High availability of the container
Automatic creation of the container
Rolling update
Deployment strategy
Docker-Swarm:
++++++++++++++
Manage and worker nodes took the work loads
1. swarm Cluster – set of nodes on which you have docker installed
2. Docker swarm is a tool that orchestrate docker containers only
3. Easily setup small config – t2.micro 1GB 1CPU core
K8S:
Master & Slave Architecture
++++++++++++++
1.	Kubernetes cluster – set of nodes/minions on which you have containers tool installed
2.	Master machine will not take any load, all the work load will take by slave only
3.	Can orchestrate any kind of  CRIO, Container-D, Rocket Container, Docker 
4.	Its so popular in market
5.	Kube or k8s , cluster you have to also setup the required CRI(Container Runtime Interface) – Docker
6.	Setup the cluster – CNI(Container N/w Interface) – like weave n/w- to perform manually
7.	K8s cluster: min 4GB RAM $ 2 CPU Core memory – GCP – free for 3 months
8.	Follow my steps to create GCP
9.	AWS – EKS – very high paid- clusters are readymade
10.	GCP – the service called as GKE – we have to create cluster
11.	Azure – AKS


Kubeadm:
Its an automatic process that install all your components in the master machine and all the components in the slave machine
Kubeadm using this process we can setup the Kubernetes cluster configurations.
As you initiate the Kubernetes cluster setup again the token will be generated and once the token is executed on the slaves, and then slaves will join the master.
Once the master and slave communication starts let’s say the administrator starts kubectl command to send a request to the master machine, when user sends a request
Master also called as Control plane

Step 1 – API Request
send user to create a container on your cluster
Request will be taken care by component called API server
API Server: is a principle component in Kubernetes, its main component in k8s its responsible to take any request from the user. This user communicate with the api server using kubectl

Step2. Scheduler:
It’s a process that is continuously monitoring the api server as a API server gets a request to create some containers, scheduler will decide where should container gets created under which slaves
Deciding task by scheduler
Scheduler will tell to api server resources are created in slave1 you can create this 5 containers on slave1
Step3: kubelet

What is kubelet?
Ans – Its an agent process
Api servers will gets to know where the containers to be created, api server will communicate to the slave machine on the slave there is agent process running called Kubelet on the slave machine.
Kubelet receive the request from API server to create 5 containers of tomcat.
 kubelet will interact with docker which is installed on it and pull the images of tomcat and create 5 replicas
Kubelet is the responsible to create and manage the containers/replicas on the slave machines
Once the replicas/containers are created kubelet will send response/report back to API servers.
That I have created the desired 5 Replicas

Step 4: Controller Manager – High Availability – on Master Machine
Then immediately controller manager on master will monitor these replicas.
It is going to monitor these replicas – for High Availability
Suppose  one replica goes down immediately one container will create on same

Step5. Etcd
It store values the data
It consists of complete information of how many slaves are there how many resources are there in slave and how many replicas are running, which application are running, which slaves are running complete information ae sored
Etcd will store complete information on cluster – like DB
Check k8s diagram in Google 

KubeProxy:
++++++
Kube proxy is n/w proxy that runs on each node in your cluster, implementing part of the Kubernetes service concepts.
Kube proxy maintain n/w rules on the node. These n/w rules allow n/w communication to your Pods from n/w session inside or outside the cluster.
Getting started – Google Cloud Platform
Now check Ready Made cluster for below steps
GCP – click on Kubernetes Engine – clusters - click on create cluster
Click on GKE Standard cluster – click on configure
The ready made cluster will give you 1 Master and 3 Slave machines
Each machine 4GB ram and 2 cpu core


 Note – kubernetes  we can not create containers directly we can create pods
 
Pod – is about layer of abstraction on your container
We are going to use Kubernetes controller for objects –
Very smallest object can Kubernetes work on pods

what is pod?

Note: - we can not create container
We can create pod only
Pod – is about layer of abstraction on your container
Each Pod we have an IP Address
Container will not have IP Address
As per the architecture if you create the pod, it should be distributed with pods
POD will never create on the master machine
Pod can create 2 ways
1.	Informative command means Single line command which is not adapt(we should not se)
2.	Create manifest files(means creation) – these are controller files or object creation files- k8s object definition files
3.	Here we can written in yaml files

Kubectl get pods
get – means to get from etcd running db on master machine

labels: 
====
labels are like to tag that you are attaching of your pod

6/9/21
+++++++++++++++

Topic:
1.Single Pod - single line command\
2.single pod - manifestfiles
3.Mutiple Pods - ReplicaSet
4.Services -
three types of services
   1. NodePort
   2.ClusterIP
   3.LoadBalancer
5.Rolling Update startegy 

kubectl run pod1 --image nginx

vim singlepod.yml
apiVersion: v1
kind: Pod
metadata:
  name: mypod1
  labels: 
   type: webserver
   app: myapp
   env: QA
spec:
  containers:
  - name: myc1
    image: nginx


execution: 
kubectl create -f singlepod.yml

Multiple Pods -creations:
-------------------
RepilcaSet: means multiple pods
it will create connection of multiple pods of same image

2 ways replicas:
===============
1.single line command
2. Manifest files

vim rc.yml

apiVersion: apps/v1
kind: ReplicaSet
metadata:
 name: myrs
 labels:
  type: webserver
spec:
 replicas: 3
 selector:         # 2 types of selectores --> 1. Equality Based selector   --> 2. Set-based selector
  matchLabels:
   type: webserver
 template:
  metadata:
   name: mypod
   labels:
    type: webserver
  spec:
   containers:
    - name: myn1
      image: nginx

		
kubectl create -f  rc.yml

Labels:
labels are key-value key: value pairs which is attached to pod, replication contoller and services. They are used for idetifyng for objects such as 
pods and replication controller .


Service Object:
===================
an abstract way to expose an application running on set of Pods as a n/w service.
with k8s you dont need to modify your application to use unfamiliar service discovery mechanism.
k8s gives Pods their own IP address and a single DNS name for set of Pods, ans can be load-balance across them

1. Node Port
2. Load Balancer
3. ClusterIP

NodePort:
A NodePort is an Open Port on evry nodes of your cluster. k8s transperntely routes incoming traffic on the NodePort to your service, even if 
your application running on a differente nodes
however a NodePort is assigned from pool of cluster-configured.
NodePort - 30000- 32767-  kubeproxy will be responsible to any of the port 

3 nodes:
routes incoming traffic 3 nodes

ymlfile
 - target port
 
 vim node.yml
 
apiVersion: v1
kind: Service
metadata:
  name: mysvc
spec:
   type: NodePort
   ports:
    - targetPort: 80
      port: 80
      nodePort: 30009
   selector:
     type: webserver

kubecl create -f node.yml

ClusterIP:
The ClusterIP provides a load-balanced IP address. ... Internally, Kubernetes resolves the label selector to a set of pods, 
and takes the ephemeral Pod IP addresses and generates Endpoints resources that the ClusterIP proxies traffic to.
 The kube-proxy that is running on each node is used to configure each worker node.

Load Balancer:
K8s service controller automates the creations  of the external load balancer, healths checks(if needed), firewall rules(if needed) and retrive the
 external ip allocated by the cloud provider and popular in the service object
 
vim lb.yml

apiVersion: v1
kind: Service
metadata:
  name: mysvc1
spec:
   type: LoadBalancer
   ports:
   - targetPort: 80
     port: 80
   selector:
     type: webserver
	 

Rolling update Strategy :
-------------------
nginx - i want to creatwe 3 replicas for nginx
scanaio: 
nginx: 1.0 - myappv1 - 3 replicas
after 15 days - nginx - 2.0 - 3 replcas 


7/9/21
+++++++++++++++++++++++++++++++++
Rolling update Strategy:

deployment.yml

kind: Deployment
apiVersion: apps/v1
metadata:
  name: kubeserve
spec:
  replicas: 3
  minReadySeconds: 10 # wait for 45 sec before going to deploy next pod
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1  
      maxSurge: 1        # max number of pods to run for the deployment
  selector:
    matchLabels:
      app: kubeserve
  template:
    metadata:
      name: kubeserve
      labels:
        app: kubeserve
    spec:
      containers:
       - name: app
         image: leaddevops/kubeserve:v1
        
---
kind: Service
apiVersion: v1
metadata:
   name: kubeserve-svc
spec:
  type: NodePort
  ports:
    - port: 80
      targetPort: 80
  selector: 
    app: kubeserve
	
kubectl create -f deployment.yml

kubectl get pods
kubectl describe pod kubeserve-6975497d98(podname) | less
kubectl rollout history deployment kubeserve
kubectl describe deployment kubeserve | less

update the image from v1 to v2:
++++++++++++++++++++++++++++
kubectl set image deployment kubeserve app=leaddevops/kubeserve:v2


Rollout Undo: Roll Back
--------------------
from new version to old version
kubectl rollout undo deployment kubeserve
kubectl rollout status deployment kubeserve
kubectl rollout history deployment kubeserve

k8s Dash Board:
+++++++++++++++++++++++++
vim k8s-dashboard.yml

apiVersion: v1
kind: Namespace
metadata:
  name: kubernetes-dashboard

---

apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: default 

---

kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: default
spec:
  type: NodePort
  ports:
    - port: 80
      targetPort: 9090
  selector:
    k8s-app: kubernetes-dashboard

---

apiVersion: v1
kind: Secret
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-csrf
  namespace: default
type: Opaque
data:
  csrf: ""

---

apiVersion: v1
kind: Secret
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-key-holder
  namespace: default
type: Opaque

---

kind: ConfigMap
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-settings
  namespace: default

---

kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: default
rules:
  # Allow Dashboard to get, update and delete Dashboard exclusive secrets.
  - apiGroups: [""]
    resources: ["secrets"]
    resourceNames: ["kubernetes-dashboard-key-holder", "kubernetes-dashboard-certs", "kubernetes-dashboard-csrf"]
    verbs: ["get", "update", "delete"]
    # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map.
  - apiGroups: [""]
    resources: ["configmaps"]
    resourceNames: ["kubernetes-dashboard-settings"]
    verbs: ["get", "update"]
    # Allow Dashboard to get metrics.
  - apiGroups: [""]
    resources: ["services"]
    resourceNames: ["heapster", "dashboard-metrics-scraper"]
    verbs: ["proxy"]
  - apiGroups: [""]
    resources: ["services/proxy"]
    resourceNames: ["heapster", "http:heapster:", "https:heapster:", "dashboard-metrics-scraper", "http:dashboard-metrics-scraper"]
    verbs: ["get"]

---

kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
rules:
  # Allow Metrics Scraper to get metrics from the Metrics server
  - apiGroups: ["metrics.k8s.io"]
    resources: ["pods", "nodes"]
    verbs: ["get", "list", "watch"]

---

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kubernetes-dashboard
subjects:
  - kind: ServiceAccount
    name: kubernetes-dashboard
    namespace: default

---

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kubernetes-dashboard
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
#  name: kubernetes-dashboard
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: kubernetes-dashboard
    #namespace: kubernetes-dashboard
    namespace: default

---

kind: Deployment
apiVersion: apps/v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: default
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      k8s-app: kubernetes-dashboard
  template:
    metadata:
      labels:
        k8s-app: kubernetes-dashboard
    spec:
      containers:
        - name: kubernetes-dashboard
          image: kubernetesui/dashboard:v2.0.3 ## kubernetesui/dashboard:v2.0.0-beta8
          ports:
            - containerPort: 9090
              protocol: TCP
          args:
            - --namespace=default
            - --enable-skip-login
            - --disable-settings-authorizer
            - --enable-insecure-login
          # Uncomment the following line to manually specify Kubernetes API server Host
          # If not specified, Dashboard will attempt to auto discover the API server and connect
          # to it. Uncomment only if the default does not work.
          # - --apiserver-host=http://my-address:port
          volumeMounts:
            # Create on-disk volume to store exec logs
            - mountPath: /tmp
              name: tmp-volume
          livenessProbe:
            httpGet:
              path: /
              port: 9090
            initialDelaySeconds: 30
            timeoutSeconds: 30
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsUser: 1001
            runAsGroup: 2001
      volumes:
        - name: tmp-volume
          emptyDir: {}
      serviceAccountName: kubernetes-dashboard
      nodeSelector:
        "beta.kubernetes.io/os": linux
      # Comment the following tolerations if Dashboard must not be deployed on master
      tolerations:
        - key: node-role.kubernetes.io/master
          effect: NoSchedule

---

kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: dashboard-metrics-scraper
  name: dashboard-metrics-scraper
  namespace: default
spec:
  ports:
    - port: 8000
      targetPort: 8000
  selector:
    k8s-app: dashboard-metrics-scraper

---

kind: Deployment
apiVersion: apps/v1
metadata:
  labels:
    k8s-app: dashboard-metrics-scraper
  name: dashboard-metrics-scraper
  namespace: default
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      k8s-app: dashboard-metrics-scraper
  template:
    metadata:
      labels:
        k8s-app: dashboard-metrics-scraper
      annotations:
        seccomp.security.alpha.kubernetes.io/pod: 'runtime/default'
    spec:
      containers:
        - name: dashboard-metrics-scraper
          image: kubernetesui/metrics-scraper:v1.0.1
          ports:
            - containerPort: 8000
              protocol: TCP
          livenessProbe:
            httpGet:
              scheme: HTTP
              path: /
              port: 8000
            initialDelaySeconds: 30
            timeoutSeconds: 30
          volumeMounts:
          - mountPath: /tmp
            name: tmp-volume
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsUser: 1001
            runAsGroup: 2001
      serviceAccountName: kubernetes-dashboard
      nodeSelector:
        "beta.kubernetes.io/os": linux
      # Comment the following tolerations if Dashboard must not be deployed on master
      tolerations:
        - key: node-role.kubernetes.io/master
          effect: NoSchedule
      volumes:
        - name: tmp-volume
          emptyDir: {}
		  
kubectl create -f k8s-dashboard.yml
slaveip address and kubernetes-dashboard portnumber type browser

Delete all POds, Service and Objects $ Deployment:
------------------------------
kubectl delete all --all

Namespace:
===============

k82, namespaces povides a mechanisam of isolating group of resouces with a single cluster, Names of resources need to be unique.
Name Spaces:
++++++++++++++++++++++++++++++++++++++++

apiVersion: apps/v1
kind: ReplicaSet
metadata:
   name: myrs
   labels: 
     type: webserver
     namespace: sobha
spec:
   replicas: 3
   selector:
      matchLabels:
         type: webserver
        template:
         metadata:
           name: mypod
           labels:
              type: webserver
   spec:
    containers:
    - name: myn1
      image: nginx

Kubernetes Probe:
+++++++++++++++++++++++++
Kubernetes has three types of probes that can be declared:

Liveness Probe: used to determine how alive the container is, regardless of dependencies. ...
Readiness Probe: used to determine whether the container is ready to accept traffic. The container can enter a non-ready state when external dependencies are failing. ...
Startup Probe: used to determine if the container started up. ...

8/9/21
++++++++++++++++++++++++

Topic:
1.Namespace
2.Storage/volums
3.Stateless
4. Statefull

difference between deployment and service in kubernetes
==================================
What's the difference between a Service and a Deployment in Kubernetes?
 A deployment is responsible for keeping a set of pods running.
 A service is responsible for enabling network access to a set of pods.
 We could use a deployment without a service to keep a set of identical pods running in the Kubernetes cluster.

Storage:
------------
it's a preserve the data

if you are storing the data on host machine locally that kind of volumes is called host path volume 
ex- /hoat/path

you are preserving the data on node which will be created that data on node1 
pv1 - 1gi


Volumes:
1. PV - Persistent Volumes - defalult volume
2. PVC - Persistent Volumes claim - 
AWS, Azure, GCP, K8S, IBM

Diff volumes:
===========
1.Sender
2.NFS - onprimises
3.EBS- AWS
4.Hostpath- GCP
5.Gluserfs

storage admin - They will be taking the responsible for free pool diff types of volumes area available & also this preserved data called a persistence volume
================

what is persistence Volume
============================

is about piece o storage in your cluster that has to be provisined by administrator

   or
Many times you can create with dynamically of pv -->means we dont have options like EBS
so for that we have to create dynamically of the components called EBS(AWS) and set of storage class

PV:
--------------

apiVersion: v1
kind: PersistentVolume
metadata:
 name: block-pv
spec:
 storageClassName: manual            #2. Standrad
 capacity: 
  storage: 1Gi
 accessModes:
  - ReadWriteOnce             #1. RWO 2. RWM 3. ROM
 hostPath:
  path: /tmp/data


PVC: persistent Volume Claim 
--------------------------------------

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
 name: pvc
spec:
 storageClassName: manual
 accessModes:
  - ReadWriteOnce
 resources:
  requests:
   storage: 1Gi

Stateless & Statefull
+++++++++++++++++++++++++++++++++
statleless Application - we can create pod -Frontnend server - storing data is not available
Statefull - we can create DB - Backend server - storing db will be available

Statless Application:
_________________
java application
3 Replicas - 3 pods
Deploymenthis applicaion usin deploymen object called stateless application


StatefulSets:
-------------------------------------
DB

Task:
----
Frontnend:
_---------- 
POD - Creation  - java/.net/python - they are not storing data this is called stateless applicaion
Application - Configuratoin

Backend:
------------
Fetching the DB from frontend

scale up:
i will create pod1 --pod10

scale down:
it will delete pod1--pod 10

Pre-requiste of create statefull appliction:
---------------------
1. 2 pods will e creating with ownip
2. 2 pvc will create
3. DNS will create


StatefullSet:


apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: "nginx"
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: k8s.gcr.io/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi
		  
Execuion:

 kubectl get pods -w -l app=nginx
 
Busybox:
kubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm /bin/sh
13/9/21
-----------

Configmaps:



apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - image: mynginx
    name: myn1
    volumeMounts:
     - name: myvol1
       mountPath: /etc/config

  volumes:
   name: myvol
   configMap:
     name: dev-config1
  restartPolicy: Never
  
Secrets:
vim secretsdemo.yml

apiVersion: v1
kind: Secret
metadta:
  name: secret2
 type: opaque
 data:
  username: YWRtaW4=
  password: YWRtaW5AMTIz
  




Secrets:
vim prodENV.yml
apiVersion: v1
kind: Pod
metadata:
  name: pod-secret12
spec:
  containers:
    - image: nginx
      name: c1
      env:
       - name: secret_Password
         valueFrom:
           secretKeyRef:
             name: secret12
             key: password


Horizontal Pod Autoscalling:
_____________________________



---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      name: nginxpod
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:latest
          resources:
            limits:
              cpu: 20m ## 10% of 1 core on your system

---

apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
spec:
  type: ClusterIP  ## this is default if we do not type in service definition
  selector:
    app: nginx
  ports:
   - protocol: TCP
     port: 80
     targetPort: 80

---

apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 10


kubectl run -i --tty load-generator --rm --image=busybox --restart=Never -- /bin/sh -c "while sleep 0.01; do wget -q -o- http://10.8.2.85; done"

























































